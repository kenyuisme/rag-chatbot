{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e7230b-8ced-4396-a0b5-8674b5897ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful\n"
     ]
    }
   ],
   "source": [
    "from chonkie import SemanticChunker, SemanticChunk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "import numpy\n",
    "from typing import List, Dict, Any\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "print(\"Import Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a999a1cd-50fc-45ee-931d-1018c7ff9d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1557ce00-4a9d-4f20-a14a-9280bbd2724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"  # Or your preferred model\n",
    "\n",
    "def process_document_chunks(doc_text: str, doc_id: str, model_name: str = \"all-MiniLM-L6-v2\") -> List[SemanticChunk]:\n",
    "    \"\"\"\n",
    "    Process a document into SemanticChunks using a SemanticChunker.\n",
    "    Returns a list of SemanticChunk objects with embeddings.\n",
    "    \"\"\"\n",
    "    encoder = SentenceTransformer(model_name)\n",
    "    \n",
    "    semantic_chunker = SemanticChunker(\n",
    "        embedding_model=embedding_model\n",
    "    )\n",
    "    \n",
    "    chunks = semantic_chunker.chunk(doc_text)\n",
    "\n",
    "    \n",
    "    print(f\"Document split into {len(chunks)} semantic chunks\")\n",
    "    return chunks\n",
    "\n",
    "def semantic_chunk_to_qdrant_point(chunk: SemanticChunk, doc_id: str, chunk_id: int, \n",
    "                                  use_sentence_vectors: bool = False) -> models.PointStruct:\n",
    "    \"\"\"\n",
    "    Convert a SemanticChunk to a Qdrant PointStruct.\n",
    "    \n",
    "    Args:\n",
    "        chunk: The SemanticChunk to convert\n",
    "        doc_id: Document identifier\n",
    "        chunk_id: Chunk identifier within the document\n",
    "        use_sentence_vectors: If True, use individual sentence embeddings as named vectors\n",
    "                             If False, use the chunk's embedding as a single vector\n",
    "    \"\"\"\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    \n",
    "    payload = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"chunk_text\": chunk.text,\n",
    "        \"sentence_texts\": [s.text for s in chunk.sentences],\n",
    "        \"sentence_start_indices\": [s.start_index for s in chunk.sentences],\n",
    "        \"sentence_end_indices\": [s.end_index for s in chunk.sentences],\n",
    "    }\n",
    "    \n",
    "    if use_sentence_vectors:\n",
    "        named_vectors = {\n",
    "            f\"sentence_{i}\": sentence.embedding.tolist()\n",
    "            for i, sentence in enumerate(chunk.sentences)\n",
    "            if sentence.embedding is not None\n",
    "        }\n",
    "        \n",
    "        return models.PointStruct(\n",
    "            id=point_id,\n",
    "            vector=named_vectors,  # Named vectors approach\n",
    "            payload=payload\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        chunk_embedding = None\n",
    "        if hasattr(chunk, 'embedding') and chunk.embedding is not None:\n",
    "            chunk_embedding = chunk.embedding.tolist()\n",
    "        elif chunk.sentences and chunk.sentences[0].embedding is not None:\n",
    "            chunk_embedding = chunk.sentences[0].embedding.tolist()\n",
    "        else:\n",
    "            # Fallback - should rarely happen if using proper SemanticChunker\n",
    "            raise ValueError(\"No embedding found for chunk or its sentences\")\n",
    "            \n",
    "        return models.PointStruct(\n",
    "            id=point_id,\n",
    "            vector=chunk_embedding,  # Single vector approach\n",
    "            payload=payload\n",
    "        )\n",
    "\n",
    "def create_standard_collection(client: QdrantClient, collection_name: str, vector_size: int = 384):\n",
    "    \"\"\"\n",
    "    Create a standard Qdrant collection with single vectors per point.\n",
    "    \"\"\"\n",
    "    collections = client.get_collections().collections\n",
    "    collection_names = [collection.name for collection in collections]\n",
    "    \n",
    "    if collection_name not in collection_names:\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"Standard collection '{collection_name}' created.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "def create_named_vectors_collection(client: QdrantClient, collection_name: str, max_sentences: int = 20):\n",
    "    \"\"\"\n",
    "    Create a Qdrant collection that supports named vectors for sentence-level embeddings.\n",
    "    \"\"\"\n",
    "    collections = client.get_collections().collections\n",
    "    collection_names = [collection.name for collection in collections]\n",
    "    \n",
    "    if collection_name not in collection_names:\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config={\n",
    "                **{f\"sentence_{i}\": models.VectorParams(size=384, distance=models.Distance.COSINE) \n",
    "                   for i in range(max_sentences)},  # Support up to max_sentences per chunk\n",
    "            }\n",
    "        )\n",
    "        print(f\"Named vectors collection '{collection_name}' created with support for {max_sentences} sentences per chunk.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "def upsert_chunks_to_qdrant(client: QdrantClient, collection_name: str, \n",
    "                           chunks: List[SemanticChunk], doc_id: str, use_sentence_vectors: bool = False):\n",
    "    \"\"\"\n",
    "    Upload chunks to Qdrant, with option to use sentence-level vectors.\n",
    "    \n",
    "    Args:\n",
    "        client: QdrantClient instance\n",
    "        collection_name: Name of the collection to upload to\n",
    "        chunks: List of SemanticChunk objects\n",
    "        doc_id: Document identifier\n",
    "        use_sentence_vectors: If True, use individual sentence embeddings as named vectors\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        point = semantic_chunk_to_qdrant_point(chunk, doc_id, i, use_sentence_vectors)\n",
    "        points.append(point)\n",
    "    \n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    \n",
    "    vector_type = \"sentence-level named vectors\" if use_sentence_vectors else \"chunk-level vectors\"\n",
    "    print(f\"Uploaded {len(points)} chunks to collection '{collection_name}' using {vector_type}\")\n",
    "\n",
    "def setup_qdrant_client():\n",
    "    \"\"\"Initialize the Qdrant client with credentials from .env file\"\"\"\n",
    "    return QdrantClient(\n",
    "        url=os.environ.get(\"QDRANT_URL\"), \n",
    "        api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    "    )\n",
    "\n",
    "def load_plain_text(file_path: str) -> str:\n",
    "    \"\"\"Utility: read a plain‚Äêtext document\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff96b8b4-239f-43d2-83cd-26383d5f2c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking 'ai in games' ...\n",
      "Document split into 32 semantic chunks\n",
      "Chunking 'CV' ...\n",
      "Document split into 9 semantic chunks\n",
      "Chunking 'marriott international' ...\n",
      "Document split into 19 semantic chunks\n",
      "Total chunks created: 60\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the \"data\" folder in the same directory as the script\n",
    "    input_folder = os.path.join(os.getcwd(), \"data\")\n",
    "    \n",
    "    # Check if the data folder exists\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Error: The folder '{input_folder}' does not exist.\")\n",
    "        exit(1)\n",
    "        \n",
    "    all_chunks = []\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        path = os.path.join(input_folder, fname)\n",
    "        doc_text = load_plain_text(path)\n",
    "        doc_id = os.path.splitext(fname)[0]\n",
    "        print(f\"Chunking '{doc_id}' ...\")\n",
    "        semantic_chunks = process_document_chunks(doc_text, doc_id, model_name=embedding_model)\n",
    "        all_chunks.extend(semantic_chunks)\n",
    "    print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f088f108-ecbf-436e-afe4-5d9b60b17f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = setup_qdrant_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9eab527-2f4b-402b-954c-25557d5d0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard collection 'ragchatbot_standard' created.\n",
      "Uploaded 60 chunks to collection 'ragchatbot_standard' using chunk-level vectors\n",
      "Named vectors collection 'ragchatbot_named_vectors' created with support for 20 sentences per chunk.\n",
      "Uploaded 60 chunks to collection 'ragchatbot_named_vectors' using sentence-level named vectors\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the Qdrant client\n",
    "    \n",
    "    # OPTION 1: Use standard collection with one vector per chunk\n",
    "    standard_collection = \"ragchatbot_standard\"\n",
    "    create_standard_collection(qdrant_client, standard_collection)\n",
    "    upsert_chunks_to_qdrant(qdrant_client, standard_collection, all_chunks, doc_id, use_sentence_vectors=False)\n",
    "    \n",
    "    # OPTION 2: Use named vectors collection with sentence-level vectors\n",
    "    named_vectors_collection = \"ragchatbot_named_vectors\"\n",
    "    create_named_vectors_collection(qdrant_client, named_vectors_collection)\n",
    "    upsert_chunks_to_qdrant(qdrant_client, named_vectors_collection, all_chunks, doc_id, use_sentence_vectors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
