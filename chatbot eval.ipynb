{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14be029f-0e65-4c01-ac05-62a98bc0ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval import evaluate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(\"Import Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f674549-9297-4e05-9e09-7a7d6adaaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebba64d-2b1f-4594-877c-984379fc6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(os.environ.get(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbe47e-fcb5-4826-8f20-cd0f0fab88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qdrant_client():\n",
    "    \"\"\"Initialize the Qdrant client with credentials from .env file\"\"\"\n",
    "    return QdrantClient(\n",
    "        url=os.environ.get(\"QDRANT_URL\"), \n",
    "        api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    "    )\n",
    "\n",
    "qdrant_client = setup_qdrant_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e41a7c-e751-4114-a8ae-6e54faf7205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [\n",
    "    {\n",
    "        \"query\": \"What method did AlphaGo Zero use for training?\",\n",
    "        \"expected_answer\": \"AlphaGo Zero used only reinforcement learning with no human-labeled data and significantly outperformed AlphaGo.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"AlphaGo was later succeeded by AlphaGo Zero, which was trained purely on reinforcement learning and was not given any labelled data such as human-played games to train on.\",\n",
    "            \"AlphaGo Zero was noted to severely outperform its predecessor, winning against AlphaGo 100-0, while also requiring less computing power and training time.\",\n",
    "            \"Considering the vast difference in performance between AlphaGo and AlphaGo Zero, it can be inferred that an AI trained only on unsupervised learning has much greater potential...\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What was special about AlphaGo’s move 37 in game 2?\",\n",
    "        \"expected_answer\": \"The move was highly unconventional, rated at 1 in 10,000 likelihood, and demonstrated the creativity of reinforcement learning beyond human precedent.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"AlphaGo played an unexpectedly brilliant move so bizarre that even experts... mistakenly thought it to have been an error at first.\",\n",
    "            \"AlphaGo calculated the probability of a human player making the same move as 1 in 10000 yet made the move anyway...\",\n",
    "            \"This was a revelation that the AI could only have made from its simulations of games playing against itself.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why was AlphaStar’s use of disruptors notable?\",\n",
    "        \"expected_answer\": \"AlphaStar adopted an advanced but rare strategy that influenced future metagame trends among professional human players.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"AlphaStar favoured a strategy using a unit called 'disruptor'... it was uncommon at the time due to the executional difficulty of the strategy.\",\n",
    "            \"Interestingly, the use of disruptors became more commonplace in later years...\",\n",
    "            \"AlphaStar plays in a similar manner to professional human players but with some unique style quirks.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What limitation does AlphaStar’s wall-off mistake reveal?\",\n",
    "        \"expected_answer\": \"It demonstrated shallow understanding of tactical strategies, compensating with inhuman precision rather than correcting foundational mistakes.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"AlphaStar did attempt a wall-off, it left a gap in the wall and thus left a vulnerable opening...\",\n",
    "            \"It likely never learned that building the wall-off incorrectly is supposed to result in a disadvantaged position.\",\n",
    "            \"As it was able to cover for the opening... it likely never recognized it as an error.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What tools and languages does the candidate know?\",\n",
    "        \"expected_answer\": \"The candidate is proficient in Python, R, SQL, Excel, Tableau, Power BI, Power Query, Think-Cell, and has experience with predictive modeling, ETL, and web scraping.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"Languages & Tools: Python | R | SQL | Tableau | Excel (Advanced) | Power BI | Power Query | Think-Cell\",\n",
    "            \"Analytics: Predictive Modeling, Statistical Inference, Optimization, Data Visualization\",\n",
    "            \"Data Engineering: JSON/XML/HTML Parsing | Web Scraping | BeautifulSoup | ETL\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the candidate’s experience with RAG systems?\",\n",
    "        \"expected_answer\": \"Designed a RAG (Retrieval-Augmented Generation) tool for document search during a product analyst role, with top-tier team performance.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"Product Analyst, Dubai Future Foundation... Designed an AI-powered RAG tool for intelligent document search...\",\n",
    "            \"...ranked Top 4 among 20+ teams.\",\n",
    "            \"Mapped UX friction points to drive feature enhancements and user engagement.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What did the candidate do during their M&A project at Ferrari?\",\n",
    "        \"expected_answer\": \"Conducted financial and strategic modeling for M&A at Ferrari, supporting executive decisions through data-backed insights.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"Modeled strategic growth and synergy scenarios to support corporate development...\",\n",
    "            \"Led cross-functional valuation and due diligence analysis (financial + strategic)...\",\n",
    "            \"Translated quantitative findings into executive-level recommendations...\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What operational risks might Marriott face in Sindalah?\",\n",
    "        \"expected_answer\": \"Supply chain dependency on a new, untested port introduces operational risks, especially given the high expectations of luxury hospitality.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"The only method of delivering any supplies to Sindalah is to go by sea.\",\n",
    "            \"There is a high chance that they will be going through the Port of NEOM in Oxagon...\",\n",
    "            \"...there is a significant chance of minor and major issues occurring in its early years.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why is alcohol a challenge for Marriott in NEOM?\",\n",
    "        \"expected_answer\": \"Legal ambiguity and sociocultural tension around alcohol could impact guest satisfaction and local acceptance.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"All importing, manufacturing, possession, and consumption of alcohol is illegal in Saudi Arabia for religious reasons.\",\n",
    "            \"Claims that sales of alcohol will be allowed in Sindalah... and also claims that they will not.\",\n",
    "            \"...Marriott faces a different challenge – how can we satisfy both the foreigners who want to consume alcohol and the locals who are against it?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why is Sindalah a unique challenge for Marriott?\",\n",
    "        \"expected_answer\": \"Sindalah’s isolation, sustainability demands, and cultural constraints make it a distinct operational and strategic challenge.\",\n",
    "        \"expected_contexts\": [\n",
    "            \"Marriott is not new to operating luxury hotels, yet doing so in Sindalah will still present new challenges...\",\n",
    "            \"Sindalah is an island with no airports...\",\n",
    "            \"Sustainability is a big topic in NEOM and thus will be expected of Sindalah and Marriott...\"\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cea171-4824-4bac-8252-591f8b86e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b54800c1714973b5b0e26bb6f5b20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea27f942-7839-4fae-a802-dd256ee8366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query: str):\n",
    "    return embedding_model.encode(query).tolist()\n",
    "\n",
    "def retrieve_relevant_chunks(query: str, top_k: int = 5):\n",
    "    query_vector = get_query_embedding(query)\n",
    "\n",
    "    search_result = qdrant_client.query_points(\n",
    "        collection_name=\"ragchatbot_standard\",\n",
    "        query=query_vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    all_chunk_texts = []\n",
    "    for _, scored_points in search_result:\n",
    "        for point in scored_points:\n",
    "            chunk_text = point.payload.get(\"chunk_text\", \"\")\n",
    "            all_chunk_texts.append(chunk_text)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_chunk_texts = list(dict.fromkeys(all_chunk_texts))\n",
    "    \n",
    "    return unique_chunk_texts\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return f\"\"\"You are a helpful assistant. Answer the question using only the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer(prompt, max_tokens=300):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1589f1a9-7716-4c7e-8455-311251b2caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        )\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_length=2500,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        return pipeline(prompt)\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39aa1bd4-b393-4287-95ea-f0e2096ddc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=2500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Write me a joke about a dog.\\nSure, here\\'s a classic joke about a dog: \"Why did the dog go to the hospital? Because he had fleas!\"'}]\n"
     ]
    }
   ],
   "source": [
    "custom_llm = CustomLlama3_8B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df0776b-9fdf-4d03-94bb-0abbe4443910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "test_cases = []\n",
    "for item in eval_data:\n",
    "    context = retrieve_relevant_chunks(item[\"query\"])\n",
    "    prompt = build_prompt(context, item[\"query\"])\n",
    "    test_cases.append(\n",
    "        LLMTestCase(\n",
    "            input=item[\"query\"],\n",
    "            actual_output=generate_answer(prompt),\n",
    "            retrieval_context=context,\n",
    "            expected_output=item[\"expected_answer\"],\n",
    "            context=item[\"expected_contexts\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "retrieval_metrics = [\n",
    "    ContextualPrecisionMetric(model=custom_llm),\n",
    "    ContextualRecallMetric(model=custom_llm),\n",
    "    ContextualRelevancyMetric(model=custom_llm)\n",
    "]\n",
    "\n",
    "generation_metrics = [\n",
    "    AnswerRelevancyMetric(model=custom_llm),\n",
    "    FaithfulnessMetric(model=custom_llm)\n",
    "]\n",
    "\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fb1b1-3041-4820-ae16-e12792e92e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_cases=test_cases, metrics=retrieval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c01247-af82-4463-8b80-2e8aea234c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_cases=test_cases, metrics=generation_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
