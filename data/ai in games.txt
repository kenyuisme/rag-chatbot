Chua Ken Yu (33205571)
AI and the Future of Work - INV-5424 - DMIB2
A1: Theoretical Individual Paper
Title: Self-learning AI in playing games
 
Introduction
In 1997, a high-stakes chess rematch was held between Deep Blue, an IBM supercomputer algorithm, and Garry Kasparov, one of the greatest chess players of all time (Yao, 2022). Deep Blue won the match and talks of the potential of AI surged. Since then, the very definition of AI has changed, let alone the technologies behind it. Deep Blue was programmed with specialised chess knowledge along with a brute-force search algorithm. While it resulted in a powerful and useful chess engine, Deep Blue is a far cry from what we consider to be AI in modern times, which is a mimicry of the human brain powered by an algorithm with self-learning capabilities allowing it to improve continuously through many iterations. Games are often used to test the abilities of such AI as games have a clearly defined goal, allowing the success of the AI to be objectively measured in plain numbers. This paper will look at the training methods and outcomes of AlphaGo and AlphaStar, two self-learning AIs both developed by DeepMind, and extract some insights from the two case studies, identifying some opportunities for improvement along with considerations for the future.

AlphaGo
Go is a turn-based board game played by 2 players with seemingly simple rules but is deceptively complex (Holcomb et al., 2018). In 2015, DeepMind, an artificial intelligence start-up bought out by Google in 2014, developed an AI named AlphaGo specifically designed to play Go (Holcomb et al., 2018). AlphaGo was trained using deep neural networks to analyse massive quantities of human-played games and employed a Monte-Carlo tree search to evaluate the current positions and identify the best moves. (Holcomb et al., 2018). By using the Monte-Carlo tree search, AlphaGo can simulate thousands of games playing against itself in the current position. Every new position is then added to the search tree and simulated until there is an outcome to the games. The value of the position is determined by the average outcome of the simulations from that position while the best move is determined by the relative value of the position (Gelly & Silver, 2011). AlphaGo was then made to play millions of games against itself to learn through reinforcement learning and had those games added into a separate neural network to analyse from using the Monte-Carlo tree search once again (Metz, 2016a). As AlphaGo had a large neural network of human-played games, it was able to determine the likelihood of a human player playing any given move independent of the value of the move (Metz, 2016a).

In 2016, AlphaGo played 5 games against Lee Sedol, one of the greatest Go players in the world, in a high-profile match and won 4-1 (Holcomb et al., 2018). Most notably, in the 37th move of the second game, AlphaGo played an unexpectedly brilliant move so bizarre that even experts of the game could not understand it and mistakenly thought it to have been an error at first (Metz, 2016b). According to Metz (2016a), AlphaGo calculated the probability of a human player making the same move as 1 in 10000 yet made the move anyway as it had determined from its simulations that the move had a high probability of winning. This was a revelation that the AI could only have made from its simulations of games playing against itself. Had AlphaGo been trained only on human games, it would not have considered a move with such a low probability of being played by human players. Using reinforcement learning, AlphaGo was able to exceed the realm of the “tried and true” and produce a new idea. 

In the 78th move of game 4 of that same match, Lee Sedol was in a losing position before he played an equally brilliant move to AlphaGo’s move in game 2, turning the game around (Metz, 2016b). AlphaGo rated the probability of a human player playing Lee’s move as 1 in 10000, the same probability for which it rated its own move (Metz, 2016b). Critically, AlphaGo then proceeded to blunder with its next few moves and ended up losing the game (Metz, 2016b). This shows that while AlphaGo was able to produce “outside the box” ideas, it was unable to react appropriately to one. 

AlphaGo was later succeeded by AlphaGo Zero, which was trained purely on reinforcement learning and was not given any labelled data such as human-played games to train on (Holcomb et al., 2018). AlphaGo Zero was noted to severely outperform its predecessor, winning against AlphaGo 100-0, while also requiring less computing power and training time (Holcomb et al., 2018).


AlphaStar
AlphaStar is an AI also created by DeepMind but for the purpose of playing Starcraft II, a computer video game first released in 2010 (The AlphaStar team, 2019). There are many differences between Starcraft and board games such as Go or Chess previously conquered by AI, the most notable of which is that Starcraft is played in real-time instead of being turn-based. As such, players, including AlphaStar, are constantly and continuously evaluating the position and adjusting their strategy at all times. In addition, Starcraft is a game of imperfect information as players are unable to view the whole state of the game at any point and must make decisions based only on the information they have available. Skills in Starcraft can be broadly broken down into two categories - macro and micro. Macro skills refer to strategic decision-making, mostly in the form of resource allocation, while micro skills refer to motor sensory skills such as the speed and accuracy of inputs. As AlphaStar operates using a direct API to the Starcraft game engine rather than through traditional input devices such as a mouse and keyboard, it skips past all motor sensory challenges and has micro capabilities far beyond any human player (Madan, 2020).

AlphaStar operates using a neural network which was initially trained using supervised learning from large quantities of human-played games, allowing AlphaStar to imitate basic strategies commonly used by players (The AlphaStar team, 2019). Different versions of AlphaStar were then made to play against each other in a continuous league where new competitors, made by branching from existing competitors, would be periodically added. The different agents would learn from their games, developing their own playstyles while also learning how to play against different strategies (The AlphaStar team, 2019). In the interest of creating unique agents, agents were given different learning objectives such as trying to beat a specific opponent or to win with a specific strategy. 

In 2019, AlphaStar played 5 games against Joona "Serral" Sotala, arguably the world’s best player at the time, and won 4-1 (ArtosisTV, 2019a). It must be stated that there were restrictions on both players during the match. AlphaStar was limited to only being able to make several hundred inputs a minute, similar to a human professional player, rather than the several thousand an AI would easily be capable of. Serral was instructed to play in a “standard” manner rather than using alternative, high-risk strategies in addition to being handicapped with a computer, mouse, and keyboard he was not familiar with. In the match, AlphaStar plays in a similar manner to professional human players but with some unique style quirks. For example, AlphaStar favoured a strategy using a unit called “disruptor.” While not necessarily a suboptimal strategy, it was uncommon at the time due to the executional difficulty of the strategy. Interestingly, the use of disruptors became more commonplace in later years, suggesting that AlphaStar may simply have been ahead of the curve.

Notably, AlphaStar is known to make a considerably basic mistake that even average players, let alone professional players, rarely make (ArtosisTV, 2019). At the beginning of the game, it is standard for players to employ a basic technique termed “wall-off” where the player positions their buildings in a certain pattern, forming a wall to restrict units from easily entering or exiting their base. As can be seen in the first game of the match, while AlphaStar did attempt a wall-off, it left a gap in the wall and thus left a vulnerable opening in its defences. Serral made repeated attempts to exploit the opening, but AlphaStar was able to perfectly repel them with impossible micro techniques that cannot be replicated by a human player. Considering the manner in which AlphaStar was trained, it is likely that AlphaStar only understood the wall-off technique on a superficial level from its neural network of human games and did not know the difference between performing the technique correctly and incorrectly. As it was able to cover for the opening in its defences with inhuman micro, it likely never learned that building the wall-off incorrectly is supposed to result in a disadvantaged position.
 Figure 1: The red circle highlights the obvious hole in AlphaStar’s wall-off (ArtosisTV 2019b)

Implications and Opportunities
AI being able to master a game has obvious implications for the game itself. Stockfish is a chess AI which is already being used to evaluate positions in a chess game, giving insights as to which player is winning as well as guiding beginners to learn the game. Outside of games, these breakthroughs can be transferred into real-world applications. 

AlphaGo was noted to play with the aim of raising the probability of winning, rather than achieving a higher margin of victory in the manner which human players do (Holcomb et al., 2018). For a human player, playing for a higher probability of winning and playing for a higher margin of victory may as well be interchangeable terms, due to the high complexity in distinguishing between the two. In actuality, basic heuristics such as “material count” are poor indicators of a position’s strength in Go (Gelly & Silver, 2011). AlphaGo is able to evaluate positions much more accurately using its Monte-Carlo Tree search. In the real world, humans can use AI in a comparable manner, albeit with much more complex algorithms, to plot what should be done to achieve their goals. As a simplified example, trying to complete a project on time is different from trying to complete a project as soon as possible, although the two can be easily conflated. AI can help us distinguish between the two and raise the probability that the project will be completed on time.

AlphaStar represents several major innovations. The first is the capability of its neural network to model exceptionally long sequences of likely actions. The AlphaStar team (2019) comments that it is often necessary to make predictions using exceedingly long sequences of data in the real world, adding that learnings from AlphaStar have contributed to solving this challenge. Secondly, the league method in which AlphaStar was trained resulted in AlphaStar being able to find a strategy which was “least likely to go wrong” (The AlphaStar team, 2019). 

Self-learning AI tends to stick to the first well-performing strategy and only focuses on refining it in further iterations rather than exploring potentially better strategies (Rex L, 2021). This can also be observed to some extent with AlphaStar where it repeatedly used the same strategy, likely the one which had the highest success rate in its training, in all of its games against Serral. Human professional players will often have a variety of strategies prepared and will pick a different one every game depending on the opponent, their condition on the day, the outcomes of the previous games, or just sheer randomness to take advantage of the incomplete information nature of Starcraft and keep themselves unpredictable. Translating to real-world scenarios, AlphaStar’s style of only picking the single best-performing strategy in every scenario would be akin to Ford’s famous one-size-fits-all strategy of only offering the Model T in black. In the VUCA world of the 21st century, it can be argued that flexibility and adaptability should be prioritised over pure optimisation. Following this line of thought, a good area of improvement could be in designing AI models which not only considers the best overall policy but also the most appropriate policy in a given situation. 

As impressive as AlphaStar was, it only managed to achieve such a prominent level in Starcraft through substantial amounts of supervised learning - initially from being fed human-played games and later from being placed in a league with multiple agents with distinct goals. AlphaStar was not able to learn the game by itself through unsupervised learning in the same manner that AlphaGo Zero was. Considering the vast difference in performance between AlphaGo and AlphaGo Zero, it can be inferred that an AI trained only on unsupervised learning has much greater potential than one with human-labeled data. As such, AlphaStar’s performance can be thought of as but the tip of the iceberg. Moving forward, it could be interesting to have more research regarding developing AI in complex games using only unsupervised learning and observing if the increase in performance remains true. If so, future AI models could benefit from having the aim of eventually creating unsupervised versions. 

Limitations of AI
AlphaGo’s loss to Lee Sedol in game 4 highlights a critical issue in self-learning AI - they are unable to respond dynamically to a new situation. Unlike humans, who are capable of thinking irrationally and thus respond to unexpected problems in innovative ways, AIs are merely mathematical models designed to find the best solution to a well-defined problem. Games are some of the very few activities where the objective, rules, and possible actions can be clearly defined in great detail. Real-world problems often have much blurrier and vague outlines. In order to effectively employ AI in solving such problems, humans must be integrated into the process to cover up for where machines are lacking.

A common criticism of AI is the unexplainable and thus unjustifiable nature of its decisions. Knowledge of how the AI was developed and trained does not necessarily shed any insights as to why it has made a certain prediction. This was seen in AlphaGo where the best explanation David Silver, who led the creation of AlphaGo, could offer for its brilliant move was simply that AlphaGo regarded it as a winning move (Metz, 2016a). This issue is exacerbated in the more complex game which AlphaStar plays. In the first game, AlphaStar spots and severely overreacts to a scouting unit from Serral by committing to bringing its units back to defend. Presumably, AlphaStar mistook Serral’s scouting unit as part of a much larger attacking force. A professional human player is very unlikely to make such a mistake. Exactly what AlphaStar was reacting to or what AlphaStar thought Serral’s actions were leading to is unknown to us. All that can be said is “AlphaStar misread the situation, and thus reacted inappropriately.” In the context of real-world AI deployment, this can pose many issues. Suppose an AI recommends building shelters in preparation for a threat. How do we know if the AI’s assessment of said threat is a reasonable one? In game 3 of the match against Serral, AlphaStar committed to a long-term investment (known as the “robotics bay”) despite the urgent need to defend against Serral’s attack. While the strategy would eventually work out as AlphaStar claims its win, questions must be raised as to whether AlphaStar accurately predicted that it had the resources to spare or if AlphaStar was simply incapable of adapting to the situation at hand, thus investing in the robotics bay at the same timing as in the other games it played.

AlphaStar’s inability to construct a wall-off represents a potential problem with the output of AI. As previously discussed, the phenomenon is likely a result of AlphaStar finding an alternative solution to building the wall-off, namely the impossible micro techniques it is capable of. It may be worth pointing out that AlphaStar was already limited in the number of inputs per minute it could make in an effort to prevent it from performing such inhuman maneuvers. Evidently, the challenge of restricting the AI to human limitations proved to be more difficult than imposing just a simple constraint. In the context of AlphaStar itself playing the game, the error may not sound like an issue, but let us imagine that AlphaStar’s task was to find optimal strategies for human players to implement instead. A player trying to follow such a strategy would be spending many fruitless hours in an attempt to recreate AlphaStar’s actions instead of what would have otherwise been the obvious move of simply constructing a wall-off. Until the day comes when humanity has robots to perform all varieties of tasks directly for us, we will likely be seeking ideas, inputs, and suggestions from AI for all but the simplest of tasks. To ensure we are not wasting our time with unrealistic ideas generated from AI, AI algorithms must be able to understand what is physically possible for a human to accomplish in greater detail.

Conclusion
Go is undoubtedly a complex game; Starcraft is even more so. DeepMind’s cutting-edge AI made breakthroughs not only in the technology industry but also in the scene of the games, showcasing strategies that were unthinkable at the time and proving the potential of AI. While undeniably impressive, AlphaGo and AlphaStar’s mastery of the game still left much to be desired. Particularly with regards to AlphaStar, there are many “what-ifs” seeding doubt as to whether the output of the AI can truly be relied on. Real-world problems are magnitudes more complicated than these games. To successfully integrate AI in the process of tackling wicked problems, we must first understand what the current limitations of AI are. Otherwise, poor, or incomplete ideas will be taken as the norm, digitised, or in the worst-case scenario, fed to a larger, more powerful algorithm and be automated. To avoid such a scenario, humans must still play a critical role in all processes to be automated by AI from designing the algorithm and defining the parameters the AI should focus on to interpreting the output of the AI and deciding how to utilise it.
 

Holcomb, S. D., Porter, W. K., Ault, S. V., Mao, G., & Wang, J. (2018, March). Overview on deepmind and its alphago zero ai. In Proceedings of the 2018 international conference on big data and education (pp. 67-71).
Gelly, S., & Silver, D. (2011). Monte-Carlo tree search and rapid action value estimation in computer Go. Artificial Intelligence, 175(11), 1856–1875. https://doi.org/10.1016/j.artint.2011.03.007
Metz, C. (2016a). How Google's AI Viewed the Move No Human Could Understand. Wired. Accessed 10 December 2023 https://www.wired.com/2016/03/googles-ai-viewed-move-no-human-understand/
Metz, C. (2016b). In Two Moves, AlphaGo and Lee Sedol Redefined the Future. Wired. Accessed 10 December 2023 https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/
Madan, C. R. (2020). Considerations for Comparing Video Game AI Agents with Humans. Challenges, 11(2), 18.
The AlphaStar Team. (2019). AlphaStar: Mastering the real-time strategy game StarCraft II. Google Deepmind. Accessed 13 December 2023 https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/
Rex L. (2021). Reinforcement Learning on Tetris. Medium. Accessed 13 December 2023 https://medium.com/mlearning-ai/reinforcement-learning-on-tetris-707f75716c37
ArtosisTV. (2019a, November 18). AlphaStar vs Serral - An Introduction. [Video]. Youtube. Accessed 14 December 2023 https://www.youtube.com/watch?v=OxseexGkv_Q&t=0s
ArtosisTV. (2019b, November 18).  AlphaStar vs Serral - Game 1. [Video]. Youtube. Accessed 14 December 2023 https://www.youtube.com/watch?v=nbiVbd_CEIA&t=0s
Yao, D. (2022). 25 Years Ago Today: How Deep Blue vs. Kasparov Changed AI Forever. AI Business. Accessed 14 December 2023 https://aibusiness.com/ml/25-years-ago-today-how-deep-blue-vs-kasparov-changed-ai-forever
